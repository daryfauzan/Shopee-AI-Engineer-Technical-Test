
## Describe differences between REST API, MCP in the context of AI.

REST API and MCP differ significantly in their design, purpose, and use within the context of AI. A REST API (Representational State Transfer) is a traditional web service architecture that relies on HTTP methods such as GET, POST, PUT, and DELETE to facilitate stateless communication between clients and servers. In AI applications, REST APIs are commonly used for model inference endpoints, retrieving or sending data, and integrating with external services. They follow a request-response pattern, typically exchanging data in JSON or XML formats, and benefit from being a well-established standard with robust tooling, documentation, and widespread adoption.

In contrast, the Model Context Protocol (MCP) is a newer protocol created specifically to address the needs of AI agent interactions. Unlike REST APIs, which are designed for general-purpose client-server communication, MCP focuses on enabling structured communication between AI systems and external tools or resources. It allows AI agents to share context more effectively and supports stateful, complex interactions that are essential for LLM-based systems to perform multi-step reasoning and tool use. MCP is therefore optimized for AI-first workflows, where maintaining context and integrating multiple resources dynamically is critical.

## How REST API, MCP, can improve the AI use case.

Both REST APIs and MCP play important roles in improving AI use cases, but they do so in different ways that reflect their design goals. REST APIs strengthen AI adoption by making integration with existing enterprise systems straightforward. They allow AI models to connect seamlessly with databases, web applications, and mobile platforms, while their stateless nature supports scalability through load balancing and horizontal scaling. By separating AI logic from application logic, REST APIs also promote modularity and fit well within a microservices architecture. Their standardized request-response model and widespread developer familiarity ensure that AI services are accessible, consistent, and easy to consume across diverse environments.

MCP, on the other hand, enhances AI use cases by directly addressing the unique needs of AI agents and workflows. It enables richer context sharing between AI systems and tools, which is essential for maintaining continuity in multi-step reasoning tasks. MCP is designed for smooth integration with external resources, such as APIs and databases, and supports orchestrating multiple AI agents working together toward a common goal. Its focus on semantic understanding and AI workflow patterns reduces communication overhead and makes interactions more efficient than general-purpose protocols. By optimizing for context-driven, tool-augmented reasoning, MCP empowers AI systems to operate more intelligently and effectively in complex, dynamic environments.


## How do you ensure that your AI agent answers correctly?

Ensuring that an AI agent answers correctly requires a combination of validation strategies, technical approaches, and continuous monitoring. One effective method is ground truth comparison, where model outputs are tested against known correct answers in benchmark datasets. In cases where absolute correctness is harder to determine, multi-model consensus can be applied by comparing results across different models to identify consistent outputs. For high-stakes applications, human-in-the-loop review processes add an additional safeguard, while confidence scoring helps quantify uncertainty so that potentially unreliable responses can be flagged.

On the technical side, advanced methods such as Retrieval-Augmented Generation (RAG) allow the AI to ground its answers in authoritative sources, improving factual accuracy. Fact-checking pipelines can further validate outputs against structured knowledge bases, while output parsing ensures responses follow the required formats and constraints. A/B testing also plays a role by comparing different model versions or prompting strategies to determine which configurations yield the most reliable results.

Finally, maintaining correctness requires ongoing monitoring and feedback. Continuous evaluation on benchmark datasets ensures performance does not degrade over time, while user feedback loops provide valuable real-world corrections that can refine the system. Error analysis helps identify recurring failure modes, guiding targeted improvements, and version control ensures that model performance is tracked across deployments. Together, these practices create a robust framework that not only improves immediate accuracy but also supports long-term reliability and trust in AI agents.

## Describe what can you do with Docker / Containerize environment in the context of AI

Using Docker and containerized environments in the context of AI provides major benefits across model deployment, development, and operations. For deployment, containers make it possible to create reproducible environments, ensuring that models run consistently across development, staging, and production. They simplify dependency management by packaging AI frameworks like PyTorch or TensorFlow with exact version requirements, while GPU-enabled containers, such as those supported by NVIDIA Docker, allow seamless execution of CUDA-based workloads. Containers also streamline model serving, enabling inference through specialized servers like TensorFlow Serving, TorchServe, or NVIDIA Triton.

From a development perspective, Docker improves productivity and flexibility by isolating different AI projects, preventing dependency conflicts between frameworks and libraries. It also supports scalability when combined with orchestration platforms like Kubernetes, which can handle auto-scaling of AI workloads in response to demand. Integration with CI/CD pipelines further ensures that models can be tested, validated, and deployed automatically, reducing friction between data science and engineering teams. Resource management is another benefit, as containers make it easier to allocate and limit CPU or GPU usage for different AI tasks.

Operationally, containerization guarantees consistency across different environments and cloud providers, enabling portability and simplifying infrastructure management. It also aligns well with microservices architectures, where complex AI systems can be broken down into smaller, containerized components that communicate efficiently. Rolling updates allow new model versions to be deployed with zero downtime, minimizing risks during upgrades. Additionally, containerized monitoring solutions make it possible to collect logs, track performance metrics, and ensure reliability in production AI systems. Altogether, Docker and containerization provide a powerful foundation for building scalable, maintainable, and production-ready AI solutions.


## How do you finetune the LLM model from raw?

Finetuning an LLM model from raw involves a structured process that spans data preparation, training approaches, technical implementation, and deployment. The first step is preparing high-quality datasets—this includes curating and cleaning domain-specific data, tokenizing it with the model’s native tokenizer, and formatting it according to the fine-tuning method (e.g., instruction-following or completion-based). A proper train/validation split is also essential to monitor generalization and avoid overfitting.

For training, several strategies are possible depending on resource availability and goals. Full fine-tuning updates all parameters but is computationally heavy, while parameter-efficient methods like LoRA, QLoRA, or adapters allow effective fine-tuning with reduced cost. Instruction tuning improves task-following behavior by training on instruction–response pairs, and reinforcement learning from human feedback (RLHF) aligns the model with user preferences through iterative reward optimization.

On the technical side, frameworks such as Hugging Face Transformers, DeepSpeed, or Axolotl provide ready-to-use tools for fine-tuning. Hardware is a critical factor, often requiring multi-GPU setups or cloud environments with large VRAM. Key hyperparameters like learning rate, batch size, and gradient accumulation need careful tuning to achieve stable training. Regular checkpointing ensures that intermediate states are saved for recovery, analysis, or future re-training.

Finally, evaluation and deployment ensure that the fine-tuned model is effective and safe. Performance is measured using perplexity, BLEU scores, or task-specific benchmarks, while safety testing checks for harmful or biased outputs. For deployment, optimization techniques like quantization, pruning, or distillation make the model more efficient and cost-effective. Version management tracks model changes over time, ensuring reproducibility and transparency in production environments.